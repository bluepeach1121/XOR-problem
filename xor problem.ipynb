{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# LeakyReLU slope\n",
    "alpha = 0.01\n",
    "\n",
    "# Initialize parameters\n",
    "def initialize_parameters():\n",
    "    W1 = np.random.randn(2, 3) * 0.01  # Weights for the first hidden layer\n",
    "    b1 = np.zeros((1, 3))              # Biases for the first hidden layer\n",
    "    W2 = np.random.randn(3, 3) * 0.01  # Weights for the second hidden layer\n",
    "    b2 = np.zeros((1, 3))              # Biases for the second hidden layer\n",
    "    W3 = np.random.randn(3, 1) * 0.01  # Weights for the output layer\n",
    "    b3 = np.zeros((1, 1))              # Biases for the output layer\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyReLU activation function\n",
    "def leaky_relu(Z):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "\n",
    "def leaky_relu_derivative(Z):\n",
    "    return np.where(Z > 0, 1, alpha)\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "# Derivative of Sigmoid\n",
    "def sigmoid_derivative(A):\n",
    "    return A * (1 - A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_propagation(X, W1, b1, W2, b2, W3, b3):\n",
    "    # First hidden layer\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = leaky_relu(Z1)\n",
    "    \n",
    "    # Second hidden layer\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = leaky_relu(Z2)\n",
    "    \n",
    "    # Output layer\n",
    "    Z3 = np.dot(A2, W3) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy Loss function\n",
    "def compute_loss(Y, A3):\n",
    "    m = Y.shape[0]\n",
    "    loss = -1/m * np.sum(Y * np.log(A3) + (1 - Y) * np.log(1 - A3))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation\n",
    "def backward_propagation(X, Y, cache):\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Output layer gradients\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = np.dot(A2.T, dZ3) / m\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "    \n",
    "    # Second hidden layer gradients\n",
    "    dA2 = np.dot(dZ3, W3.T)\n",
    "    dZ2 = dA2 * leaky_relu_derivative(Z2)\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    \n",
    "    # First hidden layer gradients\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * leaky_relu_derivative(Z1)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    gradients = (dW1, db1, dW2, db2, dW3, db3)\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters\n",
    "def update_parameters(W1, b1, W2, b2, W3, b3, gradients, learning_rate):\n",
    "    dW1, db1, dW2, db2, dW3, db3 = gradients\n",
    "    \n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W3 -= learning_rate * dW3\n",
    "    b3 -= learning_rate * db3\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the neural network\n",
    "def train(X, Y, learning_rate, epochs):\n",
    "    \n",
    "    W1, b1, W2, b2, W3, b3 = initialize_parameters()\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(epochs):\n",
    "        # Forward propagation\n",
    "        A3, cache = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(Y, A3)\n",
    "        \n",
    "        \n",
    "        gradients = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Update parameters\n",
    "        W1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, gradients, learning_rate)\n",
    "        \n",
    "       \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {i}, Loss: {loss}\")\n",
    "    \n",
    "    return W1, b1, W2, b2, W3, b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def predict(X, W1, b1, W2, b2, W3, b3):\n",
    "    A3, _ = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
    "    predictions = A3 > 0.5\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931471526434245\n",
      "Epoch 100, Loss: 0.6931471217294013\n",
      "Epoch 200, Loss: 0.6931470887913481\n",
      "Epoch 300, Loss: 0.6931470527994834\n",
      "Epoch 400, Loss: 0.6931470158494979\n",
      "Epoch 500, Loss: 0.693146978153776\n",
      "Epoch 600, Loss: 0.6931469416922685\n",
      "Epoch 700, Loss: 0.6931469000745236\n",
      "Epoch 800, Loss: 0.6931468520542392\n",
      "Epoch 900, Loss: 0.6931467960505592\n",
      "Epoch 1000, Loss: 0.6931467300319811\n",
      "Epoch 1100, Loss: 0.6931466522413835\n",
      "Epoch 1200, Loss: 0.6931465561967959\n",
      "Epoch 1300, Loss: 0.6931464387546438\n",
      "Epoch 1400, Loss: 0.6931462933615935\n",
      "Epoch 1500, Loss: 0.6931461109270728\n",
      "Epoch 1600, Loss: 0.6931458785941167\n",
      "Epoch 1700, Loss: 0.6931455777807989\n",
      "Epoch 1800, Loss: 0.6931451809667135\n",
      "Epoch 1900, Loss: 0.6931446462424655\n",
      "Epoch 2000, Loss: 0.6931439077065887\n",
      "Epoch 2100, Loss: 0.6931428577842291\n",
      "Epoch 2200, Loss: 0.6931413129461669\n",
      "Epoch 2300, Loss: 0.6931389463699429\n",
      "Epoch 2400, Loss: 0.693135197119167\n",
      "Epoch 2500, Loss: 0.6931287762306597\n",
      "Epoch 2600, Loss: 0.6931168421420981\n",
      "Epoch 2700, Loss: 0.6930918788409418\n",
      "Epoch 2800, Loss: 0.6930300575176745\n",
      "Epoch 2900, Loss: 0.6928282075523331\n",
      "Epoch 3000, Loss: 0.6917097988722147\n",
      "Epoch 3100, Loss: 0.6655962504859283\n",
      "Epoch 3200, Loss: 0.3835514288037545\n",
      "Epoch 3300, Loss: 0.017714864177086783\n",
      "Epoch 3400, Loss: 0.006326926325865376\n",
      "Epoch 3500, Loss: 0.0035302689438272993\n",
      "Epoch 3600, Loss: 0.0023705249487276396\n",
      "Epoch 3700, Loss: 0.0017472019656063194\n",
      "Epoch 3800, Loss: 0.0013669195055116083\n",
      "Epoch 3900, Loss: 0.0011142832340751132\n",
      "Epoch 4000, Loss: 0.0009360900727723857\n",
      "Epoch 4100, Loss: 0.0008014787344894551\n",
      "Epoch 4200, Loss: 0.0006991596485870332\n",
      "Epoch 4300, Loss: 0.0006183400997755766\n",
      "Epoch 4400, Loss: 0.0005529796702752983\n",
      "Epoch 4500, Loss: 0.0004994458894830519\n",
      "Epoch 4600, Loss: 0.0004544436730334718\n",
      "Epoch 4700, Loss: 0.00041639894291084473\n",
      "Epoch 4800, Loss: 0.00038375311629006614\n",
      "Epoch 4900, Loss: 0.0003557504703263614\n",
      "Predictions:\n",
      "[[False]\n",
      " [ True]\n",
      " [ True]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "W1, b1, W2, b2, W3, b3 = train(X, Y, learning_rate=0.3, epochs=5000)\n",
    "\n",
    "# Predict on the training data\n",
    "predictions = predict(X, W1, b1, W2, b2, W3, b3)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
